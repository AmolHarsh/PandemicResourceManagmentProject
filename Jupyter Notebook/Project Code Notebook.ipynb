{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc361912",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "Notebook created by Amol on 26/07/2022\n",
    "----------------------------------------------------------\n",
    "File maintained by Amol\n",
    "---------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b19c2",
   "metadata": {},
   "source": [
    "Seperate code portions can be found in the VS code folder. \n",
    "This notebook gives a cohesive environment for the coder to directly manipulate and see the results without worrying too much\n",
    "about the inside structure of the various functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1768c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "#*** means that it is important to run this block of code #\n",
    "url = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "#replace value of the path variable with the file path of the csvData.csv file in your own system\n",
    "path = #\"C:/Users/ABHA RANI/Desktop/Amol/Jupyter/RawProjectStorageFile/csvData.csv\"\n",
    "population = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7475622",
   "metadata": {},
   "source": [
    "The below super class contains functions that clean the raw dataset which we are downloading from the Github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***#\n",
    "class Super_Normalization():\n",
    "    # Declaring static url of data\n",
    "    \n",
    "    # Declare the path of the population data set\n",
    "    \n",
    "    # Declaring standard colors for all countries. These colors will be fixed across all plots\n",
    "    COLOR = {'France': '#44a2c7', 'Germany': '#f42fa2', 'Finland': '#3b2b99', 'Russia': '#f41f09', 'United Kingdom': '#a25ee1', 'Italy': '#91190a', 'Spain': '#0258bb', 'Sweden': '#2f65bf', 'Slovenia': '#d2ba0d', 'Denmark': '#a83e40', 'Estonia': '#8d3b08', 'Belgium': '#9dbeaa', 'Greece': '#5e4b98', 'Luxembourg': '#b51c57', 'Norway': '#1e5c3e', 'Switzerland': '#2106f2', 'Albania': '#53ace0', 'Austria': '#223406', 'Croatia': '#4f5026', 'Latvia': '#c923cc', 'Romania': '#ae7a50', 'North Macedonia': '#59a61f', 'Serbia': '#96510b', 'Netherlands': '#0525ba', 'Belarus': '#bc6309', 'Iceland': '#e2d4b3',\n",
    "             'Monaco': '#c28cc5', 'Ireland': '#62b93e', 'San Marino': '#668d02', 'Czechia': '#bcaab2', 'Portugal': '#4851de', 'Andorra': '#c6d06f', 'Ukraine': '#5a7c84', 'Hungary': '#1d9d53', 'Liechtenstein': '#b3a9c0', 'Faeroe Islands': '#d2eef6', 'Poland': '#a55d32', 'Gibraltar': '#bfa438', 'Bosnia and Herzegovina': '#e3cad7', 'Malta': '#2346a6', 'Slovakia': '#732bc8', 'Vatican': '#e52e53', 'Moldova': '#998396', 'Cyprus': '#400089', 'Bulgaria': '#fbe7a8', 'Kosovo': '#f2c023', 'Montenegro': '#a2c1bd', 'Lithuania': '#18a424', 'Isle of Man': '#4a1793', 'Guernsey': '#171d71', 'Jersey': '#256586'}\n",
    "\n",
    "    def __init__(self, catagory):\n",
    "        self.data_frame = df\n",
    "        self.group_by_country = ()\n",
    "        self.catagory = catagory\n",
    "        self.test_df = pd.DataFrame()\n",
    "        \n",
    "        \n",
    "\n",
    "    def Filter_Column(self):\n",
    "        filt = self.data_frame.continent == \"Europe\"\n",
    "        self.data_frame = self.data_frame.loc[filt]\n",
    "        self.data_frame.set_index(\"date\", inplace=True)\n",
    "        self.data_frame.rename(columns={\"location\": \"country\"}, inplace=True)\n",
    "        self.data_frame = self.data_frame[[\n",
    "            \"country\", \"new_cases\", \"new_deaths\", \"hosp_patients\", \"icu_patients\"]]\n",
    "\n",
    "    def Grouping_by_country(self):\n",
    "        self.group_by_country = self.data_frame.groupby(\"country\")\n",
    "\n",
    "        \n",
    "    # storing the list of countries with whole column as empty values for a specific parameter\n",
    "    def list_of_Nan_countries(self, parameter):\n",
    "\n",
    "        array_of_null_countries = []\n",
    "\n",
    "        for grouped_country_name, grouped_country_database in self.group_by_country:\n",
    "            if (len(grouped_country_database) == grouped_country_database[parameter].isnull().sum()):\n",
    "                array_of_null_countries.append(grouped_country_name)\n",
    "        return array_of_null_countries\n",
    "\n",
    "    \n",
    "    # dataframe of countries for a particular parameter in the form of Pivot Table\n",
    "    def get_country_df_for_particular_parameter(self, parameter):\n",
    "        country_df_particular_parameter = self.data_frame.pivot(\n",
    "            index=\"date\", columns=\"country\", values = parameter)\n",
    "        return country_df_particular_parameter\n",
    "\n",
    "    \n",
    "    # remove the columns of the list of Nan countries from the dataframe\n",
    "    def delete_Nan_countries_from_df(self, array_of_null_countries, country_df_particular_parameter):\n",
    "\n",
    "        for df_country in country_df_particular_parameter.columns:\n",
    "            for array_country in array_of_null_countries:\n",
    "                if array_country in df_country:\n",
    "                    del country_df_particular_parameter[df_country]\n",
    "        return country_df_particular_parameter\n",
    "\n",
    "    \n",
    "    # storing the name of the parameters from the data frame of a list\n",
    "    def getparameter_array(self):\n",
    "        col_array = []\n",
    "        for col_name in self.data_frame.columns:\n",
    "            if(col_name != \"date\" and col_name != \"country\"):\n",
    "                col_array.append(col_name)\n",
    "        return col_array\n",
    "\n",
    "    \n",
    "    # returns the final clean dataset for each parameter in the form of dictionary\n",
    "    # where the parameter are the keys and data frames are the values\n",
    "    def get_final_df_Dictionary(self, rolling_days = 14):\n",
    "        self.Filter_Column()\n",
    "        self.data_frame.reset_index(inplace=True)\n",
    "        self.Grouping_by_country()\n",
    "        dictionary = {}\n",
    "        col_array = self.getparameter_array()\n",
    "        for parameter in col_array:\n",
    "            if parameter == self.catagory:\n",
    "                Nan_ans = self.list_of_Nan_countries(parameter)\n",
    "                df_ans = self.get_country_df_for_particular_parameter(parameter)\n",
    "                deleted_nan_country_df = self.delete_Nan_countries_from_df(\n",
    "                    Nan_ans, df_ans)\n",
    "                # final missing values \"inside\" the dataframe are filled using linear interpolation method\n",
    "                interpolated_df = deleted_nan_country_df.interpolate(\n",
    "                    limit_area=\"inside\")\n",
    "                \n",
    "                self.test_df = interpolated_df\n",
    "                print(interpolated_df)\n",
    "                \n",
    "                \n",
    "                #droping specific speical columns whoes data is not in the population database\n",
    "                del_Col_list = ['Kosovo', 'Jersey', 'Guernsey']\n",
    "                \n",
    "                for country in del_Col_list:\n",
    "                    if country in interpolated_df.columns:\n",
    "                        interpolated_df = interpolated_df.drop([country], axis = 1)\n",
    "                        \n",
    "                    \n",
    "                 # dividing by the population\n",
    "                divided_by_population_df = self.Dividing_by_population(interpolated_df)\n",
    "                \n",
    "                print(divided_by_population_df)\n",
    "                \n",
    "                #finding the rolling _average for better visualisation\n",
    "                rolling_avg_df = self.rolling_average(divided_by_population_df, rolling_days)\n",
    "              \n",
    "                dictionary[parameter] = rolling_avg_df \n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    \n",
    "    def Dividing_by_population(self, interpolated_df):\n",
    "        \n",
    "        for index, row in population.iterrows():\n",
    "            country_name_in_population_df  = row[\"name\"]\n",
    "            population_value = row[\"pop\"]\n",
    "            if country_name_in_population_df in interpolated_df.columns:\n",
    "                interpolated_df.loc[:, country_name_in_population_df] = interpolated_df.loc[:, country_name_in_population_df].apply(lambda x: x/population_value)\n",
    "                \n",
    "        return interpolated_df\n",
    "            \n",
    "\n",
    "    # The Type argument is the type of Normalization\n",
    "    # The catagory argument is a catagory such as 'new_cases'\n",
    "    # The countries = None arguments plots all countries if there are no specified countries\n",
    "    def plot_data_frame(self, DataFrame, Type, catagory, countries=None, rolling_days = 14, free_pass = False):\n",
    "        \n",
    "        #DataFrame = self.rolling_average(DataFrame, rolling_days)\n",
    "        if not free_pass:\n",
    "            DataFrame.reset_index(inplace=True)\n",
    "            DataFrame['date'] = pd.to_datetime(DataFrame['date'])\n",
    "        \n",
    "\n",
    "        if countries != None :\n",
    "            for column in countries:\n",
    "                if column == 'date':\n",
    "                    continue\n",
    "                if column in self.test_df.columns:\n",
    "                    plt.plot(\n",
    "                    DataFrame.date, DataFrame[column], color=Super_Normalization.COLOR[column], label=column )\n",
    "                #plt.xticks(DataFrame.date[::100])\n",
    "        if countries == None:\n",
    "            for column in DataFrame:\n",
    "                if column == 'date':\n",
    "                    continue\n",
    "                if column in self.test_df.columns:\n",
    "                    plt.plot(DataFrame['date'], DataFrame[column], color=Super_Normalization.COLOR[column], label=column)\n",
    "                #plt.xticks(DataFrame.date[::100])\n",
    "        plt.title(\"Normalizing each country with \" +\n",
    "                  Type + \" Maximum \" + catagory + \"\\n\" + \"Frame Size 120\")\n",
    "        plt.xlabel(\"Dates\")\n",
    "        plt.ylabel(\"Normalized to 1\")\n",
    "        plt.xticks(DataFrame.date[::90])\n",
    "        plt.tick_params(axis='x', labelrotation=90)\n",
    "        #plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "        #plt.legend()\n",
    "        plt.savefig(\"Raaju1234567\" +'.png', dpi = 300)\n",
    "        plt.show()\n",
    "        return DataFrame\n",
    "        \n",
    "    \n",
    "    # new_df = dataframe with date as index and countries as column\n",
    "    # rolling_days = number of days you want to take average of (strongly recommended that the value should be a multiple of 7)\n",
    "    def rolling_average(self, new_df, rolling_days):\n",
    "        \n",
    "        row_count = new_df.shape[0]\n",
    "        column_count = new_df.shape[1]\n",
    "\n",
    "        temp_data_frame = new_df.copy()\n",
    "\n",
    "        for country_index in range(column_count):\n",
    "\n",
    "            index_counter = 0\n",
    "\n",
    "            # for each date in each country\n",
    "            for date_index in range(row_count):\n",
    "\n",
    "                # surpass all the Nan values in the dataframe then only proceeding\n",
    "                if not (np.isnan(new_df.iloc[date_index, country_index])):\n",
    "\n",
    "                    # summing of the rolling days from the copied data frame\n",
    "                    # assuming that the dataframe is larger than the rolling_days parameter\n",
    "                    rolling_counter_index = date_index\n",
    "                    ending_rolling_counter_index = date_index + rolling_days\n",
    "\n",
    "                    # finding the mean of all the days within the rolling days window\n",
    "                    rolling_days_mean = temp_data_frame.iloc[rolling_counter_index:\n",
    "                                                             ending_rolling_counter_index, country_index].mean()       \n",
    "                    \n",
    "                    # updating the new value in the original dataframe\n",
    "                    new_df.iloc[ending_rolling_counter_index - 1, country_index] = rolling_days_mean\n",
    "\n",
    "                    # reached the end of the dataframe\n",
    "                    if(ending_rolling_counter_index - 1 == row_count-1):\n",
    "                        break\n",
    "\n",
    "                    # this function will work for the first number of rolling days \n",
    "                    # except the last day where we are actually filling the new average value\n",
    "                    if index_counter < rolling_days -1:\n",
    "                        # removing (here filling with Nan value) the first rolling days values from the dataframe\n",
    "                        new_df.iloc[date_index, country_index] = np.nan\n",
    "                        index_counter += 1\n",
    "        return new_df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f81264",
   "metadata": {},
   "source": [
    " ## Normalisation Techniques\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43264e",
   "metadata": {},
   "source": [
    "You can run any one block of code\n",
    "1. Global Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_Static(Super_Normalization):\n",
    "    # Initiailizig super class and this class\n",
    "    # also assigning a catagory to each instant. Such as 'new_cases'\n",
    "    def __init__(self, catagory):\n",
    "        Super_Normalization.__init__(self, catagory)\n",
    "        self.catagory = catagory\n",
    "        self.Dataframe_with_countries_as_column = Super_Normalization.get_final_df_Dictionary(self)[\n",
    "            self.catagory]\n",
    "\n",
    "    # Function divides each data point by the global maximum\n",
    "    # uses the applymap method which acts on each data point in the data set\n",
    "    def Divide_by_global_max(self):\n",
    "        global_max = self.Dataframe_with_countries_as_column.max().max()\n",
    "        self.Dataframe_with_countries_as_column = self.Dataframe_with_countries_as_column.applymap(\n",
    "            lambda x: x/global_max)\n",
    "        \n",
    "    # Function plots the new cases from specified country normalized to the global maximum\n",
    "    def plot_data_frame(self, countries=None):\n",
    "        self.Dataframe_with_countries_as_column = super().plot_data_frame(\n",
    "            self.Dataframe_with_countries_as_column, \"Global Static\", self.catagory, countries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba06373",
   "metadata": {},
   "source": [
    "2. Global Dynamic Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Global_Dynamic(Super_Normalization):\n",
    "    # Initiailizing super class and this class\n",
    "    # also assigning a catagory to each instant. Such as 'new_cases'\n",
    "    def __init__(self, catagory):\n",
    "        Super_Normalization.__init__(self, catagory)\n",
    "        self.catagory = catagory\n",
    "        self.Dataframe_with_countries_as_column = Super_Normalization.get_final_df_Dictionary(self)[\n",
    "            self.catagory]\n",
    "        #creating a column of matrix of size (row size * 1)\n",
    "        self.max_array = [0]*self.data_frame.shape[0]\n",
    "\n",
    "    # Creates the maximum array\n",
    "    # The maximum array holds global maximum value for all the frames that a particular \n",
    "    # data point was under corresponding to a particular date (row) and country (column) \n",
    "    def Create_max_array(self, frame_size):\n",
    "        \n",
    "        row_size = self.Dataframe_with_countries_as_column.shape[0]\n",
    "        \n",
    "        for i in range(0, row_size):\n",
    "            \n",
    "            # condition for the last frame\n",
    "            if i > row_size - frame_size:\n",
    "                \n",
    "                break\n",
    "                \n",
    "                \n",
    "            \n",
    "            #getting the maximum value\n",
    "            #there should not be +1 in frame size as we looking for 90 days worth of dataset not 91\n",
    "            global_frame_max = self.Dataframe_with_countries_as_column.iloc[i: i + frame_size+1].max(\n",
    "            ).max()\n",
    "            \n",
    "            #updating the matrix with the global max value for that window if that value is the greatest of all\n",
    "            for j in range(i, i + frame_size+1):\n",
    "                if self.max_array[j] < global_frame_max:\n",
    "                    self.max_array[j] = global_frame_max\n",
    "\n",
    "    # Function divides each data point by the global maximum\n",
    "    def Divide_by_max_array(self):\n",
    "        row_size = self.Dataframe_with_countries_as_column.shape[0]\n",
    "        for i in range(0, row_size):\n",
    "            self.Dataframe_with_countries_as_column.iloc[i] = self.Dataframe_with_countries_as_column.iloc[i]/self.max_array[i]\n",
    "\n",
    "    # Function plots the new cases from specified country normalized to the global maximum\n",
    "    def plot_data_frame(self, countries=None):\n",
    "        super().plot_data_frame(\n",
    "            self.Dataframe_with_countries_as_column, \"Global Dynamic\", self.catagory, countries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9faec3",
   "metadata": {},
   "source": [
    "3. Global Dynamic Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_Dynamic_Mean(Super_Normalization):\n",
    "    # Initiailizing super class and this class\n",
    "    # also assigning a catagory to each instant. Such as 'new_cases'\n",
    "    def __init__(self, catagory):\n",
    "        Super_Normalization.__init__(self, catagory)\n",
    "        self.catagory = catagory\n",
    "        self.Dataframe_with_countries_as_column = Super_Normalization.get_final_df_Dictionary(self)[\n",
    "            self.catagory]\n",
    "\n",
    "    # Function divides and adds each data points of frame by the global maximum\n",
    "    # uses the applymap method which acts on each data point in the data set\n",
    "\n",
    "    def Divide_by_max_and_add(self, frame_size):\n",
    "        row_size = self.Dataframe_with_countries_as_column.shape[0]\n",
    "        temp_data_frame = self.Dataframe_with_countries_as_column.copy()\n",
    "        \n",
    "        # setting each element of the data frame with the value of 0\n",
    "        self.Dataframe_with_countries_as_column = self.Dataframe_with_countries_as_column.applymap(lambda x: 0)\n",
    "        \n",
    "        for i in range(1, row_size):\n",
    "            \n",
    "            # condition for the last frame\n",
    "            if i > row_size-frame_size:\n",
    "                global_dynamic_max = temp_data_frame.iloc[i:row_size].max(\n",
    "                ).max()\n",
    "                self.Dataframe_with_countries_as_column.iloc[i:row_size] += temp_data_frame[i:row_size].applymap(\n",
    "                    lambda x: x/global_dynamic_max)\n",
    "                continue\n",
    "               \n",
    "            # finding the global maximum within that frame\n",
    "            global_dynamic_max = temp_data_frame.iloc[i:i+ frame_size+1].max().max()\n",
    "            \n",
    "            # the apply function acts on each country's column's data point in the dataframe\n",
    "            self.Dataframe_with_countries_as_column.iloc[i:i+frame_size + 1] += temp_data_frame.iloc[i:i+frame_size+1].applymap(lambda x: x/global_dynamic_max)\n",
    "\n",
    "    # Function takes the mean of each data point according to the number of times vaules have been added to it\n",
    "    def Divide_by_frame_size(self, frame_size):\n",
    "        row_size = self.Dataframe_with_countries_as_column.shape[0]\n",
    "        count_foward = 1\n",
    "        for i in range(0, row_size):\n",
    "            \n",
    "            # Condition for the data points within the first frame\n",
    "            # ??????Shouldn't the first frame columns be also dropped\n",
    "            if i < frame_size:\n",
    "                \n",
    "                # dividing all the countries for the same date with the frame size\n",
    "                self.Dataframe_with_countries_as_column.iloc[i] = self.Dataframe_with_countries_as_column.iloc[i]/count_foward\n",
    "                count_foward += 1\n",
    "                continue\n",
    "                \n",
    "            self.Dataframe_with_countries_as_column.iloc[i] = self.Dataframe_with_countries_as_column.iloc[i]/frame_size\n",
    "            \n",
    "        self.Dataframe_with_countries_as_column.drop(self.Dataframe_with_countries_as_column.index[row_size-(frame_size+1):row_size], inplace=True)\n",
    "\n",
    "        \n",
    "    # Function plots the new cases from each country normalized to the global maximum\n",
    "    def plot_data_frame(self, countries=None):\n",
    "        super().plot_data_frame(\n",
    "            self.Dataframe_with_countries_as_column, \"Global Dynamic Mean\", self.catagory, countries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce35a2",
   "metadata": {},
   "source": [
    "**Main Running Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***#\n",
    "\n",
    "frame_size = 120\n",
    "i = 0\n",
    "\n",
    "'''These are the list of countries that we are working with''' \n",
    "List = ['Russia',\n",
    "'Germany','United Kingdom','France','Italy','Spain','Ukraine','Poland','Romania','Netherlands','Belgium','Czechia','Greece','Sweden','Portugal','Hungary',\n",
    "'Belarus','Austria','Switzerland','Serbia','Bulgaria','Denmark','Finland','Norway','Slovakia','Ireland','Croatia','Moldova','Bosnia and Herzegovina','Albania','Lithuania','North Macedonia','Slovenia','Latvia','Estonia',\n",
    "'Cyprus','Luxembourg','Montenegro','Malta','Iceland','Isle of Man','Andorra','Faeroe Islands','Monaco','Liechtenstein','San Marino','Gibraltar',\n",
    "'Vatican']\n",
    "\n",
    "\"\"\"\n",
    "Global Static Technique\n",
    "\n",
    "To run it just uncomment the below lines of code\n",
    "\n",
    "\"\"\"\n",
    "# Catagory = [\"new_cases\", \"new_deaths\", \"hosp_patients\", \"icu_patients\"]\n",
    "# Ldm = Global_Static(Catagory[i])\n",
    "# Ldm.Divide_by_global_max()\n",
    "# Ldm.plot_data_frame()\n",
    "\n",
    "\"\"\"\n",
    "Global Dynamic Max\n",
    "\n",
    "To run it just uncomment the below lines of code\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Catagory = [\"new_cases\", \"new_deaths\", \"hosp_patients\", \"icu_patients\"]\n",
    "# country = [\"France\", \"Germany\", \"Italy\"]\n",
    "# Ldm = Global_Dynamic(Catagory[i])\n",
    "# Ldm.Create_max_array(90)\n",
    "# Ldm.Divide_by_max_array()\n",
    "# Ldm.plot_data_frame()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Global Dynamic Mean\n",
    "\n",
    "To run it just uncomment the below lines of code\n",
    "\n",
    "\"\"\"\n",
    "# Catagory = [\"new_cases\", \"new_deaths\", \"hosp_patients\", \"icu_patients\"]\n",
    "# Ldm = Global_Dynamic_Mean(Catagory[i])\n",
    "# Ldm.Divide_by_max_and_add(frame_size)\n",
    "# Ldm.Divide_by_frame_size(frame_size)\n",
    "# Ldm.plot_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc159b",
   "metadata": {},
   "source": [
    "*to see the plot of a specific country*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ldm.plot_data_frame([\"United Kingdom\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d049fc",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166911b",
   "metadata": {},
   "source": [
    "(Probability Transition Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4886a986",
   "metadata": {},
   "source": [
    "**Below functions will help you in generation of probability transition matrix through different techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d12612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sliding_technique_matrix_formation (frame_size, data_frame, country_list):\n",
    "    \n",
    "    \n",
    "    #deep copying the orginal data frame\n",
    "    signed_dataframe = data_frame.copy()\n",
    "    \n",
    "    for country in country_list:\n",
    "        \n",
    "        #intialising variables\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "        sliding_value_of_each_frame = 7 #days\n",
    "        country_val_series = data_frame[country]\n",
    "        length_of_the_series = country_val_series.shape[0]\n",
    "        \n",
    "        #finding the number of leftover days: sure to be less than 7 days\n",
    "        remaining_unanalysed_days = (length_of_the_series - frame_size) % sliding_value_of_each_frame\n",
    "\n",
    "        for slide in range(0,length_of_the_series, sliding_value_of_each_frame):\n",
    "            \n",
    "            start_index = slide\n",
    "            end_index = slide + frame_size\n",
    "            \n",
    "            if end_index > length_of_the_series:\n",
    "                break\n",
    "            \n",
    "            #finding the min and max value from the country's column for that particular frame\n",
    "            min_val_in_series = country_val_series.iloc[start_index : end_index].min()\n",
    "            max_val_in_series = country_val_series.iloc[start_index : end_index].max()\n",
    "            \n",
    "            #finding the threshold values for dividing that frame's country's values into three groups. \n",
    "            bins = np.linspace(min_val_in_series, max_val_in_series, 4)\n",
    "            group_names = [\"-\",\"+\",\"++\"]\n",
    "            \n",
    "            #updating the frame in the new signed data_frame with the group names using the cut function from pandas\n",
    "            signed_dataframe[country].iloc[start_index : end_index] = pd.cut(data_frame[country].iloc[start_index : end_index], bins, labels = group_names, include_lowest = True)\n",
    "            # Intervals are like this [a,b],(b,c], (c,d] for -, + and ++\n",
    "            \n",
    "            print(\"Country Name: \", country)\n",
    "            print(\"From data: \", data_frame[\"date\"][start_index] , \" to \", data_frame[\"date\"][end_index - 1])\n",
    "            probability_transition_matrix_generator(signed_dataframe[country][start_index : end_index], start_index, end_index)\n",
    "            \n",
    "def probability_transition_matrix_generator(country_series_values, start_index, end_index):\n",
    "    \n",
    "    #creating a numpy matrix\n",
    "    transition_matrix = np.matrix([[0.0,0,0],\n",
    "                                   [0,0,0],\n",
    "                                   [0,0,0]])\n",
    "    \n",
    "    \n",
    "    #iterating and comparing each value of the series\n",
    "    #- 1 because in the call the country's index on the right hand side in the parameter is exclusive\n",
    "    for index in range(start_index, end_index - 1):\n",
    "\n",
    "        if country_series_values[index] == \"-\" and country_series_values[index+1] == \"-\":\n",
    "            transition_matrix[0,0] += 1\n",
    "            \n",
    "        elif country_series_values[index] == \"-\" and country_series_values[index+1] == \"+\":\n",
    "            transition_matrix[0,1] += 1  \n",
    "\n",
    "        elif country_series_values[index] == \"-\" and country_series_values[index+1] == \"++\":\n",
    "            transition_matrix[0,2] += 1\n",
    "\n",
    "        elif country_series_values[index] == \"+\" and country_series_values[index+1] == \"-\":\n",
    "            transition_matrix[1,0] += 1\n",
    "\n",
    "        elif country_series_values[index] == \"+\" and country_series_values[index+1] == \"+\":\n",
    "            transition_matrix[1,1] += 1\n",
    "\n",
    "        elif country_series_values[index] == \"+\" and country_series_values[index + 1] == \"++\":\n",
    "            transition_matrix[1,2] += 1\n",
    "\n",
    "        elif country_series_values[index] == \"++\" and country_series_values[index + 1] == \"-\":\n",
    "            transition_matrix[2,0] += 1\n",
    "\n",
    "        elif country_series_values[index] == \"++\" and country_series_values[index + 1] == \"+\":\n",
    "            transition_matrix[2,1] += 1\n",
    "\n",
    "        elif country_series_values[index] == \"++\" and country_series_values[index + 1] == \"++\":\n",
    "            transition_matrix[2,2] += 1 \n",
    "\n",
    "    sum_of_columns = transition_matrix.sum(axis = 1)\n",
    "    transition_matrix[0,:] = transition_matrix[0,:]/sum_of_columns[0,0]\n",
    "    transition_matrix[1,:] = transition_matrix[1,:]/sum_of_columns[1,0]\n",
    "    transition_matrix[2,:] = transition_matrix[2,:]/sum_of_columns[2,0]\n",
    "\n",
    "    print(\"Transition Matrix: \", \"\\n\\n\", transition_matrix, \"\\n\")\n",
    "\n",
    "def customised_probability_transition_matrix_generator(data_frame, country_list ):\n",
    "    \n",
    "    for country in country_list:\n",
    "        val_series = data_frame[country]\n",
    "        start_index = 0\n",
    "        end_index = data_frame.shape[0] - 1\n",
    "        print(country, \" \\n\")\n",
    "        probability_transition_matrix_generator(val_series, start_index, end_index)\n",
    "        \n",
    "    \n",
    "def creating_signed_dataframe(data_frame):\n",
    "    \n",
    "    signed_data_frame = data_frame.copy()\n",
    "    \n",
    "    for country in data_frame.columns:\n",
    "        if country == \"date\": continue\n",
    "            \n",
    "        country_column = data_frame[country]\n",
    "        min_val = country_column.min()\n",
    "        max_val = country_column.max()\n",
    "        \n",
    "        #tranforming the unsigned dataframe into signed dataframe\n",
    "        signed_data_frame[country] = assigning_sign_values(min_val, max_val, data_frame, country)\n",
    "    \n",
    "    return signed_data_frame\n",
    "\n",
    "def assigning_sign_values(min_val, max_val, data_frame, country):\n",
    "    \n",
    "    #finding the threshold values for dividing that frame's country's values into three groups. \n",
    "    bins = np.linspace(min_val, max_val, 4)\n",
    "    group_names = [\"-\",\"+\",\"++\"]\n",
    "            \n",
    "    #dividing into groups using the cut function from pandas\n",
    "    return pd.cut(data_frame[country], bins, labels = group_names, include_lowest = True)\n",
    "    # Intervals for the groups are like-- [a,b],(b,c], (c,d] for -, + and ++\n",
    "\n",
    "def creating_weightage_data_frame (signed_data_frame):\n",
    "    \n",
    "    #doing this so that the date's column does not get converted also\n",
    "    signed_data_frame.set_index(\"date\", inplace = True)\n",
    "    weightage_signed_data_frame = signed_data_frame.applymap(lambda x: sign_to_weight_val_converter(x))\n",
    "    weightage_signed_data_frame.reset_index(inplace = True)\n",
    "    return weightage_signed_data_frame\n",
    "\n",
    "\n",
    "def sign_to_weight_val_converter(val):\n",
    "    \n",
    "    if val == \"-\":\n",
    "        return -1\n",
    "    elif val == \"+\":\n",
    "        return 0\n",
    "    elif val == \"++\":\n",
    "        return 1\n",
    "    \n",
    "def creating_weekly_transition_dataframe(average_weeks_in_days, weightage_signed_data_frame):\n",
    "    \n",
    "    row = weightage_signed_data_frame.shape[0]\n",
    "    column = weightage_signed_data_frame.columns\n",
    "    end_index = math.floor(row/average_weeks_in_days)\n",
    "    week_counter = 0\n",
    "    weightage_signed_data_frame['date'] = weightage_signed_data_frame['date'].astype(str)\n",
    "    \n",
    "    #creating a new dataframe of size 1/average_weeks_in_days th of the weightage signed data frame\n",
    "    weekly_transition_dataframe = pd.DataFrame(index = range(0, end_index), columns = column)\n",
    "    \n",
    "    \n",
    "    for col in column:\n",
    "        \n",
    "        if col == \"date\": continue\n",
    "            \n",
    "        col_val = weightage_signed_data_frame[col]\n",
    "        row_size = weightage_signed_data_frame.shape[0]\n",
    "        \n",
    "        for x in range(0, row_size, average_weeks_in_days):\n",
    "            start_index = x\n",
    "            end_index = x + average_weeks_in_days\n",
    "            \n",
    "            if end_index > row_size: break\n",
    "                \n",
    "            weekly_transition_dataframe['date'][x/average_weeks_in_days] = weightage_signed_data_frame['date'][end_index - 1]\n",
    "            \n",
    "            if any (np.isnan(weightage_signed_data_frame[col][start_index:end_index])):\n",
    "                 continue\n",
    "                \n",
    "            \n",
    "            val = round(weightage_signed_data_frame[col][start_index:end_index].mean())\n",
    "            \n",
    "            if val == -1:\n",
    "                weekly_transition_dataframe[col][x/average_weeks_in_days] = \"-\"\n",
    "            elif val == 0:\n",
    "                weekly_transition_dataframe[col][x/average_weeks_in_days] = \"+\"\n",
    "            elif val == 1:\n",
    "                weekly_transition_dataframe[col][x/average_weeks_in_days] = \"++\"\n",
    "            \n",
    "    \n",
    "    weekly_transition_dataframe['date'] = pd.to_datetime(weekly_transition_dataframe['date'])\n",
    "    return weekly_transition_dataframe\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94b5ea",
   "metadata": {},
   "source": [
    "**Main Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4233fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***#\n",
    "\n",
    "original_df = Ldm.Dataframe_with_countries_as_column.copy()\n",
    "\n",
    "#removing empty rows\n",
    "original_df.drop([0,1], axis=0, inplace=True)\n",
    "original_df.drop([\"index\"], axis=1, inplace=True)\n",
    "original_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Technique 1:\n",
    "output will give you probability transition matrix of each country using the sliding technique on the complete orignal database\n",
    "(uncomment the below lines and run to unleash their power)\n",
    "(you change or add countries in the list)\n",
    "\"\"\"\n",
    "#sliding_technique_matrix_formation(frame_size = 150, data_frame = original_df , country_list = ['France', 'Germany', \"United Kingdom\"])\n",
    "\n",
    "\"\"\"\n",
    "Technique 2:\n",
    "output will give you probability transition matrix of country using the sliding technique on a weekly average database\n",
    "(uncomment the below lines once by selecting all the below lines and clicking  ctrl + / and then run to unleash their power)\n",
    "(you can also add more countries in the list)\n",
    "\"\"\"\n",
    "# #the output dataframe has been catagorised into three groups of -,+, and ++ \n",
    "# signed_dataframe = creating_signed_dataframe(original_df)\n",
    "# #the output dataframe has -1,0,1 instead of -,+,++\n",
    "# weightage_signed_df = creating_weightage_data_frame(signed_dataframe)\n",
    "\n",
    "# #the output dataframe will take average of contiguous days' values and then store it as an single unit\n",
    "# #e.g avg of first 7 days and then store the mean value as week 1, next 7 days and storing it as week 2\n",
    "# weekly_transition_df = creating_weekly_transition_dataframe(average_weeks_in_days = 10, weightage_signed_data_frame = weightage_signed_df)\n",
    "# print(weekly_transition_df[\"Germany\"][0:150])\n",
    "# #display the probability transition matrix of the countries mentioned in the list\n",
    "# customised_probability_transition_matrix_generator(weekly_transition_df, country_list = [\"France\",\"Germany\", \"United Kingdom\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
